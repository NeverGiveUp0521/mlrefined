{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.  Dealing with large state spaces\n",
    "\n",
    "Notice one major fundamental difference (in terms of the RL components of each) between the cart-pole, lunar lander, chess, and pac-man examples and the various grid-world examples discussed previously: the size of the state-space.  For example\n",
    "\n",
    "- With the chess example: the number of possible configurations of chess pieces on the board is enormous - i.e., the number of states or size of state space - is on the order of $10^{120}$.  That is far larger than the number of atoms in the known universe!\n",
    "\n",
    "\n",
    "- With the cart-pole example: each of the four descriptors of the environment is continuous (or finely discretized in practice) - thus the number of states is - technically speaking - infinite.  \n",
    "\n",
    "\n",
    "- With the Pac-man example: if we use raw pixels as input then one state is a single frame of the game, and we have as many states as there are ways to legally arrange pixels in the game.  This is an extremely large number.  \n",
    "\n",
    "\n",
    "In all of these cases the (very large) size of the state space presents practical problems.  Why?  Because remember that - practically speaking - in order to learn the $Q$ properly we need to visit a representative sample of states a number of times.(theoretically speaking - we have to be able to cycle through *every possible state* infinitely often).  Again - practically speaking then - even a state space on the order of chess $10^{120}$ is far too large for us to be able to learn a representative $Q$ effectively.  \n",
    "\n",
    "Even more pragmatically, recall from the first notebook in this series that we store the $Q$ function as a matrix of size $N \\times M$, where $N$ and $M$ are the sizes of the state and action spaces respectively.  But the size of $N$ is - for the problems listed above is far too large (or infinite) to store in memory.\n",
    "\n",
    "In sum the issue here is that in most RL problems the **size of the state space** makes directly computing and storing the $Q$ function/matrix impossible.  In this section we discuss the crucial role *function approximators* (neural networks, trees, and kernels) play in ameliorating this issue, allowing us to extend the Reinforcement paradigm to problems with infinitely large state spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1  Compactly representing state space of the $Q$ function\n",
    "\n",
    "We need a way to compactly represent *the vertical or state-space dimension* of $Q$ when we have a very large (and possibly infinite number) of states.  Instead of trying to compute and record every input/output pair - which is impossible once the input space is too large or infinite - we can try the next best thing: represent $Q$ as a set of *parameterized functions* and simply fit each to their respective sample input/output data.\n",
    "\n",
    "Does the concept of \"fitting a parameterized function to a set of input/output data\" sound familiar?  That is just a fancy way of saying that we will use **regression** - a fundamental machine learning tool.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2  Using regression to represent the columns of Q\n",
    "\n",
    "- NEED TO SHOW MATRIX AGAIN I THINK, TO MOTIVATE THE IDEA OF SUB-ING OUT COLUMN WITH FUNCTION\n",
    "\n",
    "\n",
    "- PICTURE OF REGRESSION - BOTH IN 2-D AND 3-D - PROBABLY NOT TERRIBLE HERE.  JUST RE-USE / ADAPT FROM REGRESSION.\n",
    "\n",
    "\n",
    "- SLOW DOWN - EXPLAIN NOTATION\n",
    "\n",
    "Instead of storing the entire matrix $Q$, we will substitute a parameterized function for each of its columns - independently.   The $k^{th}$ available action $a_k$ has a corresponding slice of $Q$ (an $N \\times 1$ column vector)\n",
    "\n",
    "$Q_k=\\left[\\begin{array}{c}\n",
    "Q\\left(\\mathbf{f}(\\sigma_{1}),\\alpha_{k}\\right)\\\\\n",
    "Q\\left(\\mathbf{f}(\\sigma_{2}),\\alpha_{k}\\right)\\\\\n",
    "\\vdots\\\\\n",
    "Q\\left(\\mathbf{f}(\\sigma_{N}),\\alpha_{k}\\right)\n",
    "\\end{array}\\right]$\n",
    "\n",
    "As with the $Q$ matrix itself - the length of this vector is far too long to completely compute or store.  Note too that with the action fixed at $\\alpha_{k}$ every entry of this column can be thought of as a single *data point*, an input/output pair.  For example the $i^{th}$ data point is $(\\mathbf{f}(\\sigma_{i}),Q\\left(\\mathbf{f}(\\sigma_{i}),\\alpha_{k}\\right))$.  \n",
    "\n",
    "So in other words, to represent $Q_k$ we need a parameterized function $h_{\\boldsymbol{\\theta}_{k}}$ (with corresponding parameters stored in the vector ${\\boldsymbol{\\theta}_{k}}$). $h_{\\boldsymbol{\\theta}_{k}}$ takes a state $s$ as input and returns its (approximate) Q value (assuming the action is fixed at $\\alpha_{k}$).\n",
    "\n",
    "For example, suppose we have $P$ features - i.e., $\\mathbf{f}$ is a $P$ length vector $\\mathbf{f}(s) = (~f_1(s)~,f_2(s)~,...,f_P(s)~)$. Then the simplest - but still commonly used - parameterized representation is a linear combination of the input features\n",
    "\n",
    "\n",
    "$h_{\\boldsymbol{\\theta}_{k}}(s) = <\\boldsymbol{\\theta}_{k}, \\mathbf{f}(s)> =\\theta_{k,1}\\,f_{1}\\left(s\\right)+\\theta_{k,2}\\,f_{2}\\left(s\\right)+\\cdots+\\theta_{k,P}\\,f_{P}\\left(s\\right)$\n",
    "\n",
    "In the very simplest instance each feature here is just a variable of the state (an entry of the feature vector $\\mathbf{f}(s)$), so this is a linear combination of the entries of the state feature vector.  \n",
    "\n",
    "However this combination can be quite complicated.  The features may be real quantities derived from the states themselves and - since this is a regression problem - we can employ any **mathematical basis** we wish e.g., polynmoial, Fourier, decision trees, neural networks making the above highly nonlinear.  In this case each $f_p$ can be a parameterized basis function like e.g., a neural network basis element.\n",
    "\n",
    "But regardless of the basis / feature type chosen the hope here is that - when properly tuned - this model accurately represents what would be the true $Q$ values as\n",
    "\n",
    "$h_{\\boldsymbol{\\theta}_{k}}(s) \\approx Q\\left(\\mathbf{f}(s),\\alpha_{k}\\right))$.\n",
    "\n",
    "While we now need to tune these parameters, the benefit is quite plain: instead of storing the original $N$-dimensional column vector $Q_k$ we need only store the parameter values in $\\boldsymbol{\\theta}_{k}$ - e.g., in the simplest case of a linear approximator we have only $P$ values. \n",
    "\n",
    "Finally note that since $Q$ originally has $M$ columns and we are replacing each with a parameterized model, we will have $M$ total parameterized models representing $Q$.  These models are almost always taken to have the same form (i.e., all are either linear, neural networks, kernels, etc.), so we can distinguish them solely via their parameter index $h_{\\boldsymbol{\\theta}_{1}}(s)~,h_{\\boldsymbol{\\theta}_{2}}(s),\\cdots~,h_{\\boldsymbol{\\theta}_{M}}(s)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3  The Q-learning update step\n",
    "\n",
    "Remember that with the original Q-learning update discussed in Section 2.2 we update a single entry of the matrix at each step as\n",
    "\n",
    "$Q\\left(s_{k-1},\\,a_{k}\\right)=r_{k}+\\gamma\\cdot\\underset{i=1...M}{\\text{maximum}}\\,\\,Q\\left(s_{k},\\,\\alpha_{i}\\right)$\n",
    "\n",
    "\n",
    "Now, however, we have a parameterized function taking the place of each column of $Q$, and because of this we need to split this step *into two pieces* when adapting it to our current situation.  \n",
    "\n",
    "In the first step we generate the update quantity:  \n",
    "\n",
    "$q_{k}=r_{k}+\\gamma\\cdot\\underset{i=1...M}{\\text{maximum}}\\,\\,h_{\\boldsymbol{\\theta}_{i}}(s_k)$\n",
    "\n",
    "\n",
    "This is precisely what we did previously - the only thing that has changed is that now our $Q$ matrix is represented by a list of parameterized functions - one for each column - and so the *maximum* in the above update step is over our functions (and not the literal columns of a matrix).  Because we have no $Q$ matrix of values we call the value on the left hand side $q_{k}$.\n",
    "\n",
    "But we also need to update our representation of $Q$ with this data point.  In the original case, where we had no function approximators, we simply replaced the entry from the appropriate column of the $Q$ matrix with this value.  The analog here is that we now need to *re-tune* the parameters of the parameterized function taking the place of the appropriate column of $Q$.  \n",
    "\n",
    "The best way to do this is to refit the appropriate model to this data point via solving a regularized Least Squares problem.  Letting $j$ denote the index of the maximum acheived on the right hand side of the update above, an unregularized Least Squares would mean finding the parameters $\\boldsymbol{\\theta}_{j}$ that minimize the quantity $(h_{\\boldsymbol{\\theta}_{j}}(s_k)-q_{k})^{2}$.  However minimizing this alone would overfit our parameters to the single point $q_k$, and we would lose all of the information contained in the previously tuned parameters.  In order to avoid this we can add a simple regularizer that ensures that the new parameters aren't too different from the previous ones: denoting the previous set of parameters $\\boldsymbol{\\theta}_{j}^{\\text{ prev}}$ we add an appropriate regularizer to the Least Squares\n",
    "\n",
    "$\\underset{\\boldsymbol{\\theta}_{j}}{\\text{minimize}}\\,\\,\\left(h_{\\boldsymbol{\\theta}_{j}}\\left(s_{k}\\right)-q_{k}\\right)^{2}+\\beta\\left\\Vert \\boldsymbol{\\theta}_{j}-\\boldsymbol{\\theta}_{j}^{\\text{ prev}}\\right\\Vert _{2}^{2}$\n",
    "\n",
    "\n",
    "Here $\\beta$ is a parameter we can tune to ensure that the new parameters are either very close to the previously calculated parameters (by setting $\\beta$ to a large positive number), or lighten this restriction by setting $\\beta$ low.  Typically this is either set to be a small constant, or increased gradually at each episode of simulation.\n",
    "\n",
    "Regardless of the $\\beta$ setting, and the form of $h$, this regularized Least Squares problem has a closed form solution.  Setting the gradient in $\\boldsymbol{\\theta}_{j}$ above to zero and solving gives the solution\n",
    "\n",
    "$\\boldsymbol{\\theta}_{j}=\\boldsymbol{\\theta}_{j}^{\\text{ prev}}-\\frac{1}{\\beta}(h_{\\boldsymbol{\\theta}_{j}}\\left(s_{k}\\right)-q_{k})\\nabla h_{\\boldsymbol{\\theta}_{j}}\\left(s_{k}\\right)$\n",
    "\n",
    "Due to the form of this update, this is often interpreted as a stochastic gradient descent step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 An algorithm for Q learning with function approximators\n",
    "\n",
    "All together a basic algorithm for Q learning with function approximators looks like\n",
    "\n",
    "------\n",
    "Initialize parameter vectors $\\boldsymbol{\\theta}_{1},...,\\boldsymbol{\\theta}_{M}$, choose a value for $\\gamma \\in [0,1]$, and a step size rule\n",
    "\n",
    "**for** e = 1...E (the maximum number of episodes)\n",
    "    \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Select a random initial state $s_0$ and set $k=1$\n",
    "    \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **while** end state not reached AND maximum iteration count not met\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;select the action $a_k$ at random and record the resulting state $s_k$ and corresponding reward $r_k$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$j=\\underset{i=1...M}{\\text{argmax}}\\,\\,h_{\\boldsymbol{\\theta}_{i}}(s_k)$\n",
    "\n",
    "\n",
    "    \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$q_{k}=r_{k}+\\gamma\\cdot h_{\\boldsymbol{\\theta}_{j}}(s_k)$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\boldsymbol{\\theta}_{j}^{\\text{ prev}} \\longleftarrow \\boldsymbol{\\theta}_{j}$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\boldsymbol{\\theta}_{j}=\\boldsymbol{\\theta}_{j}^{\\text{ prev}}-\\frac{1}{\\beta}(h_{\\boldsymbol{\\theta}_{j}}\\left(s_{k}\\right)-q_{k})\\nabla h_{\\boldsymbol{\\theta}_{j}}\\left(s_{k}\\right)$\n",
    "    \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$k \\longleftarrow k+1$\n",
    "\n",
    "-----\n",
    "\n",
    "Due to the randomness involved in choosing states this procedure can produce results that vary significantly in quality from one run to the next.  Because of this one typically runs this method a number of times, saving the set of parameters associated with the best run.  \n",
    "\n",
    "We use this procedure to train an agent for the cart-pole problem in the Python cell below. When you run this cell the first few episodes of training will be animated - much as we did with the grid world example in Section 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-12-06 12:32:26,885] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q-learning process complete, best number of average steps in test runs = 462.5\n"
     ]
    }
   ],
   "source": [
    "from my_cartpole import my_cartpole\n",
    "test = my_cartpole()\n",
    "test.qlearn(gamma=0.8,num_episodes = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can expose the Q learning function here by activating the Python cell below - it maches the pseudo-code given above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test.qlearn??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5  What is the best action per state given a learned Q function?\n",
    "\n",
    "Given the parameterized functions representing $Q$ are learned properly, how should the agent move at each state?  Precisely as we did when with problems like grid world - where we chose the action maximizing the corresponding $Q$ value.  The only difference here is that each column of $Q$ is a parameterized function.\n",
    "\n",
    "So at a given state $s_{k-1}$ we take the action $a_k = \\alpha_j$ where the argument $j$ is given as \n",
    "\n",
    "$j=\\underset{i=1...M}{\\text{argmax}}\\,\\,h_{\\boldsymbol{\\theta}_{i}}(s_{k-1})$\n",
    "\n",
    "This is precisely how the learned agent controls the cart-pole, as you can see by activating the Python cell below.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run a few test runs of cart-pole using the trained function approximators\n",
    "test.animate_test_run()\n",
    "test.animate_test_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To expose the controller activate the Python cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test.animate_test_run??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Next version to dos:\n",
    "\n",
    "I.  General discussion of when using RL is worth using - vs engineering things by hand\n",
    "\n",
    "- the cost of simulation + cost of RL feature engineering + RL algorithm engineering vs cost of hand engineering solutions\n",
    "\n",
    "\n",
    "II.  Examples using nonlinear function approximators\n",
    "\n",
    "- fourier basis\n",
    "- deep nets / conv nets\n",
    "    \n",
    "III. Section on Implementation tips and tricks\n",
    "\n",
    "Here we can talk about the many engineering choices one has to speed up learning when of an RL algorithm like Q learning with function approximators.  e.g.,\n",
    "\n",
    "- step length selection\n",
    "\n",
    "- exploration / exploitation trade-off\n",
    "\n",
    "- memory replay\n",
    "\n",
    "- etc.,"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
